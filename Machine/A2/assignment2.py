# -*- coding: utf-8 -*-
"""Assignment_2_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QNewNVZ2OIAGWT9oG5Cjt-qTlBeCRubT
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import export_text

weather_dataset_path = 'weather_forecast_data.csv'
weather_dataset = pd.read_csv(weather_dataset_path)

weather_dataset_path = 'weather_forecast_data.csv'
weather_dataset = pd.read_csv(weather_dataset_path)


print("Initial Data Overview:")
print(weather_dataset.head())

print(weather_dataset.isnull().sum())
print(f"\nDataset shape before dropping rows with missing values: {weather_dataset.shape}")
data_dropped = weather_dataset.dropna()
print(f"\nDataset shape after dropping rows with missing values: {data_dropped.shape}")

Mean_replaced_dataset = weather_dataset.copy()
for column in weather_dataset.select_dtypes(include=['float64', 'int64']).columns:
    value_mean = weather_dataset[column].mean()
    Mean_replaced_dataset[column] = Mean_replaced_dataset[column].fillna(value_mean)
if 'Rain' in Mean_replaced_dataset.columns:
    Mean_replaced_dataset['Rain'] = Mean_replaced_dataset['Rain'].fillna('unknown')

print("\nDataset after replacing missing values with the mean:")
print(Mean_replaced_dataset.shape)
print("\nMissing values count after replacement:")
print(Mean_replaced_dataset.isnull().sum())

print("Feature Statistics Before Scaling:")
print(weather_dataset.describe())

datasets = {'Drop Missing Data': data_dropped, 'Fill Missing Data': Mean_replaced_dataset}
def evaluate_model(true_target, pred_target):
    accuracy = accuracy_score(true_target, pred_target)
    precision = precision_score(true_target, pred_target)
    recall = recall_score(true_target, pred_target)
    return accuracy, precision, recall

classifier_of_DecisionTree = DecisionTreeClassifier(criterion='entropy', random_state=42)
classifier_of_Knn = KNeighborsClassifier(n_neighbors=3)
classifier_of_NaïveBayes = GaussianNB()

def knn_from_scratch(train_features,test_features, Target_train_encoded, k=3):
    predictions = []
    for test_point in test_features.values:
        distances = np.linalg.norm(train_features.values - test_point, axis=1)  # Calculate Euclidean distances
        nearest_neighbors_indices = distances.argsort()[:k]
        nearest_neighbors_labels = Target_train_encoded[nearest_neighbors_indices]
        most_common = np.bincount(nearest_neighbors_labels).argmax()

        predictions.append(most_common)

    return predictions

for handling, dataset in datasets.items():
    print(f"\nEvaluation for handling missing data with {handling}:")

    Features = dataset.drop(columns=['Rain'])
    Target = dataset['Rain']
    encoder = LabelEncoder()
    Target = encoder.fit_transform(Target)

    train_features, test_features, train_target, test_target = train_test_split(Features, Target, test_size=0.20, random_state=42)
    scaler = StandardScaler()
    train_features_scaled = pd.DataFrame(scaler.fit_transform(train_features), columns=train_features.columns)
    test_features_scaled = pd.DataFrame(scaler.transform(test_features), columns=test_features.columns)

    classifier_of_DecisionTree.fit(train_features_scaled, train_target)
    classifier_of_Knn.fit(train_features_scaled, train_target)
    classifier_of_NaïveBayes.fit(train_features_scaled, train_target)

    Prediction_of_DecisionTree = classifier_of_DecisionTree.predict(test_features_scaled)
    Prediction_of_Knn = classifier_of_Knn.predict(test_features_scaled)
    Prediction_of_NaïveBayes = classifier_of_NaïveBayes.predict(test_features_scaled)

    accuracy_of_DecisionTree, precision_of_DecisionTree, recall_of_DecisionTree = evaluate_model(test_target, Prediction_of_DecisionTree)
    accuracy_of_Knn, precision_of_Knn, recall_of_Knn = evaluate_model(test_target, Prediction_of_Knn)
    accuracy_of_NaïveBayes, precision_of_NaïveBayes, recall_of_NaïveBayes = evaluate_model(test_target, Prediction_of_NaïveBayes)
    knn_custom_predictions = knn_from_scratch(train_features, test_features,train_target, k=3)

    print(f"Decision Tree - Accuracy: {round(accuracy_of_DecisionTree, 3)}, Precision: {round(precision_of_DecisionTree, 3)}, Recall: {round(recall_of_DecisionTree, 3)}")
    print(f"kNN - Accuracy: {round(accuracy_of_Knn, 3)}, Precision: {round(precision_of_Knn, 3)}, Recall: {round(recall_of_Knn, 3)}")
    print(f"Naïve Bayes - Accuracy: {round(accuracy_of_NaïveBayes, 3)}, Precision: {round(precision_of_NaïveBayes, 3)}, Recall: {round(recall_of_NaïveBayes, 3)}")

    print(f"\nComparison between custom kNN and scikit-learn kNN:")
    k_values = [1, 3, 5, 7, 9]
    print("===================================")
    for k in k_values:
        knn_custom_predictions = knn_from_scratch(train_features_scaled, test_features_scaled, train_target, k)
        custom_knn_accuracy, custom_knn_precision, custom_knn_recall = evaluate_model(test_target, knn_custom_predictions)
        print(f"Custom kNN (k={k}) - Accuracy: {round(custom_knn_accuracy, 3)}, Precision: {round(custom_knn_precision, 3)}, Recall: {round(custom_knn_recall, 3)}")

    print("===================================")
    for k in k_values:
        Knn = KNeighborsClassifier(n_neighbors=k)
        Knn.fit(train_features_scaled, train_target)
        Prediction_of_Knn =  Knn.predict(test_features_scaled)
        accuracy_of_Knn, precision_of_Knn, recall_of_Knn = evaluate_model(test_target, Prediction_of_Knn)
        print(f"Scikit-learn kNN (k={k}) - Accuracy: {round(accuracy_of_Knn, 3)}, Precision: {round(precision_of_Knn, 3)}, Recall: {round(recall_of_Knn, 3)}")

# Plot Decision Tree
plt.figure(figsize=(10,6))
plot_tree(classifier_of_DecisionTree, filled=True, feature_names=Features.columns, class_names=encoder.classes_)
plt.title('Decision Tree Classifier Visualization')
plt.show()

tree_rules = export_text(classifier_of_DecisionTree, feature_names=Features.columns.tolist())
print("Decision Tree Rules:")
print(tree_rules)

def describe_tree(node=0, depth=0):
    if classifier_of_DecisionTree.tree_.feature[node] == -2:
        print(f"{'  ' * depth}Leaf node: Predicted class: {encoder.classes_[np.argmax(classifier_of_DecisionTree.tree_.value[node])]} ")
        return

    feature = Features.columns[classifier_of_DecisionTree.tree_.feature[node]]
    threshold = classifier_of_DecisionTree.tree_.threshold[node]
    left_child = classifier_of_DecisionTree.tree_.children_left[node]
    right_child = classifier_of_DecisionTree.tree_.children_right[node]

    print(f"{'  ' * depth}Node {node}: {feature} <= {threshold:.2f}")

    # Describing the left and right child nodes
    print(f"{'  ' * depth}Left child of node {node}:")
    describe_tree(left_child, depth + 1)
    print(f"{'  ' * depth}Right child of node {node}:")
    describe_tree(right_child, depth + 1)
describe_tree()

print("\nDecision Tree Visualization - After Dropping Missing Values:")
plt.figure(figsize=(10,3))
plot_tree(
    classifier_of_DecisionTree,
    filled=True,
    feature_names=Features_dropped.columns,
    class_names=encoder.classes_
)
plt.title('Decision Tree Classifier - Dropped Missing Values')
plt.show()